# Spark mongoDb properties
spark.mongodb.source.uri=mongodb://localhost:27017
spark.mongodb.source.database=test
spark.mongodb.source.collection=weather_archive

# TODO: add separate settings for Postgres to support both as input source
# Spark jdbc properties
#spark.jdbc.source.url=jdbc:mysql://localhost:3306/weather_archive
#spark.jdbc.source.username=root
#spark.jdbc.source.password=admin
#spark.jdbc.source.driver-class-name=com.mysql.cj.jdbc.Driver
#spark.jdbc.source.table-name=daytemperature


spark.jdbc.source.url=jdbc:postgresql://localhost:5432/weather_archive
spark.jdbc.source.username=postgres
spark.jdbc.source.password=postgres
spark.jdbc.source.driver-class-name=org.postgresql.Driver
spark.jdbc.source.table-name=daytemperature

# Rabbit properties
rabbitmq.queue-name=q.sync-weather-queue
rabbitmq.exchange-name=x.weather-archive-exchange
rabbitmq.routing-key=sync-weather
